{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1247188a5193a0bb99f176fea36dc594d283160e"
   },
   "source": [
    "# <div style=\"text-align: center\">The Data Scientist’s Toolbox Tutorial - 2</div>\n",
    "\n",
    "### <div style=\"text-align: center\">Quite Practical and Far from any Theoretical Concepts</div>\n",
    "<img src='https://chengotto.com/wp-content/uploads/2018/02/images.duckduckgo2-600x338.jpg'>\n",
    "<div style=\"text-align:center\">last update: <b>11/21/2018</b></div>\n",
    "\n",
    "\n",
    ">###### you may  be interested have a look at it: [**The Data Scientist’s Toolbox Tutorial - 1**](https://www.kaggle.com/mjbahmani/the-data-scientist-s-toolbox-tutorial-1)\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------\n",
    "you can Fork and Run this kernel on Github:\n",
    "> ###### [ GitHub](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    " **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated**\n",
    " \n",
    " -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0a45d14ee727bf2f88a7cd0ba5e6aa338977d10b"
   },
   "source": [
    " <a id=\"top\"></a> <br>\n",
    "## Notebook  Content\n",
    "1. [Introduction](#1)\n",
    "    1. [Import](#2)\n",
    "    1. [Version](#3)\n",
    "1. [Sklearn](#4)\n",
    "    1. [Data Collection](#5)\n",
    "    1. [Framework](#6)\n",
    "    1. [Applications](#7)\n",
    "    1. [How to use Sklearn Data Set? ](#8)\n",
    "    1. [Loading external data](#9)\n",
    "    1. [Model Deployment](#10)\n",
    "    1. [Families of ML algorithms](#11)\n",
    "    1. [Prepare Features & Targets](#12)\n",
    "    1. [Accuracy and precision](#13)\n",
    "    1. [Estimators](#14)\n",
    "    1. [Predictors](#15)\n",
    "    1. [K-Nearest Neighbours](#16)\n",
    "    1. [Radius Neighbors Classifier](#17)\n",
    "    1. [Logistic Regression](#18)\n",
    "    1. [Passive Aggressive Classifier](#19)\n",
    "    1. [Naive Bayes](#20)\n",
    "    1. [BernoulliNB](#21)\n",
    "    1. [SVM](#22)\n",
    "    1. [Nu-Support Vector Classification](#23)\n",
    "    1. [Linear Support Vector Classification](#24)\n",
    "    1. [Decision Tree](#25)\n",
    "    1. [ExtraTreeClassifier](#26)\n",
    "    1. [Neural network](#27)\n",
    "        1. [What is a Perceptron?](#28)\n",
    "        1. [The XOR Problem](#29)\n",
    "    1. [RandomForest](#30)\n",
    "    1. [Bagging classifier ](#31)\n",
    "    1. [AdaBoost classifier](#32)\n",
    "    1. [Gradient Boosting Classifier](#33)\n",
    "    1. [Linear Discriminant Analysis](#34)\n",
    "    1. [Quadratic Discriminant Analysis](#35)\n",
    "    1. [Kmeans](#36)\n",
    "        1. [Plot classification probability](#37)\n",
    "1. [NLTK](#38)\n",
    "    1. [Tokenizing sentences](#39)\n",
    "    1. [NLTK and arrays](#40)\n",
    "    1. [NLTK stop words](#41)\n",
    "    1. [NLTK – stemming](#42)\n",
    "    1. [NLTK speech tagging](#43)\n",
    "    1. [Natural Language Processing – prediction](#44)\n",
    "    1. [nlp prediction example](#45)\n",
    "1. [Read more](#46)\n",
    "1. [conclusion](#47)\n",
    "1. [References](#48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec7344e7f2a1bafa9a44a518722fcd8ec47c374b"
   },
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "# 1-Introduction\n",
    "This Kernel is mostly for **beginners**, and of course, all **professionals** who think they need to review  their  knowledge.\n",
    "Also, this is  the second version for (  [The Data Scientist’s Toolbox Tutorial - 1](https://www.kaggle.com/mjbahmani/the-data-scientist-s-toolbox-tutorial-1) ) and we will continue with other important packages in this kernel.keep following!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4e28cde75726e3617dc80585626f7f8a1297a9e4"
   },
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "##   1-1 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import get_dummies\n",
    "import plotly.graph_objs as go\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import warnings\n",
    "import sklearn\n",
    "import scipy\n",
    "import numpy\n",
    "import json\n",
    "import nltk\n",
    "import sys\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c3c434ac82d771c5549c4f36d0e8e878489f252"
   },
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "## 1-2 Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "72fdff866b7cbe404867e82f9122e16fc33facf2"
   },
   "outputs": [],
   "source": [
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('sklearn: {}'.format(sklearn.__version__))\n",
    "print('scipy: {}'.format(scipy.__version__))\n",
    "print('seaborn: {}'.format(sns.__version__))\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('Python: {}'.format(sys.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4284a92f8326eb09dccf0a795f44931c5a7487cc"
   },
   "source": [
    "## 1-3 Setup\n",
    "\n",
    "A few tiny adjustments for better **code readability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "f9a265dce077fd183b2172378a85ed2d23290189"
   },
   "outputs": [],
   "source": [
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('white')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cedecea930b278f86292367cc28d2996a235a169"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e80040de557789b0dff267ce45ba3e494885fee"
   },
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "# 2- Sklearn \n",
    "- The __open source__ Python ecosystem provides __a standalone, versatile and powerful scientific working environment__, including: [NumPy](http://numpy.org), [SciPy](http://scipy.org), [IPython](http://ipython.org), [Matplotlib](http://matplotlib.org), [Pandas](http://pandas.pydata.org/), _and many others..._\n",
    "\n",
    "\n",
    "\n",
    "- Scikit-Learn builds upon NumPy and SciPy and __complements__ this scientific environment with machine learning algorithms;\n",
    "- By design, Scikit-Learn is __non-intrusive__, easy to use and easy to combine with other libraries;\n",
    "- Core algorithms are implemented in low-level languages.\n",
    "\n",
    "## 2-1 Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "666c206f83175114a513b37fb9ae322b5cd8543e"
   },
   "source": [
    "**Supervised learning**:\n",
    "\n",
    "1. Linear models (Ridge, Lasso, Elastic Net, ...)\n",
    "1. Support Vector Machines\n",
    "1. Tree-based methods (Random Forests, Bagging, GBRT, ...)\n",
    "1. Nearest neighbors \n",
    "1. Neural networks (basics)\n",
    "1. Gaussian Processes\n",
    "1. Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "44eef8d741beebe15555c5166360b2ce77f5d5b1"
   },
   "source": [
    "**Unsupervised learning**:\n",
    "\n",
    "1. Clustering (KMeans, Ward, ...)\n",
    "1. Matrix decomposition (PCA, ICA, ...)\n",
    "1. Density estimation\n",
    "1. Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8da2cc5428b697a7b5f21d34038d343bb8b094bb"
   },
   "source": [
    "__Model selection and evaluation:__\n",
    "\n",
    "1. Cross-validation\n",
    "1. Grid-search\n",
    "1. Lots of metrics\n",
    "\n",
    "_... and many more!_ (See our [Reference](http://scikit-learn.org/dev/modules/classes.html))\n",
    "<a id=\"5\"></a> <br>\n",
    "## 2-2 Data Collection\n",
    "**Data collection** is the process of gathering and measuring data, information or any variables of interest in a standardized and established manner that enables the collector to answer or test hypothesis and evaluate outcomes of the particular collection.[techopedia]\n",
    "\n",
    "**Iris dataset**  consists of 3 different types of irises’ (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray\n",
    "\n",
    "The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.[6]\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9269ae851b744856bce56840637030a16a5877e1"
   },
   "outputs": [],
   "source": [
    "# import Dataset to play with it\n",
    "dataset = pd.read_csv('../input/iris-dataset/Iris.csv')\n",
    "train=pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\n",
    "test=pd.read_csv('../input/quora-insincere-questions-classification/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "58ed9c838069f54de5cf90b20a774c3e236149b3"
   },
   "source": [
    "**<< Note 1 >>**\n",
    "\n",
    "* Each row is an observation (also known as : sample, example, instance, record)\n",
    "* Each column is a feature (also known as: Predictor, attribute, Independent Variable, input, regressor, Covariate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b5fd1034cd591ebd29fba1c77d342ec2b408d13"
   },
   "source": [
    "After loading the data via **pandas**, we should checkout what the content is, description and via the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "edd043f8feb76cfe51b79785302ca4936ceb7b51"
   },
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "368a939dcb26a9e956261309cb0ea9d479f3ac8c"
   },
   "outputs": [],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8a877d51d20c1ad31bb635cffc89175426eb77c"
   },
   "source": [
    "<a id=\"6\"></a> <br>\n",
    "# 2-3 Framework\n",
    "\n",
    "Data comes as a finite learning set ${\\cal L} = (X, y)$ where\n",
    "* Input samples are given as an array $X$ of shape `n_samples` $\\times$ `n_features`, taking their values in ${\\cal X}$;\n",
    "* Output values are given as an array $y$, taking _symbolic_ values in ${\\cal Y}$.\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bafb45df9ecfe90563f2f9a1be8a327823cf6d35"
   },
   "source": [
    "The goal of supervised classification is to build an estimator $\\varphi: {\\cal X} \\mapsto {\\cal Y}$ minimizing\n",
    "\n",
    "$$\n",
    "Err(\\varphi) = \\mathbb{E}_{X,Y}\\{ \\ell(Y, \\varphi(X)) \\}\n",
    "$$\n",
    "\n",
    "where $\\ell$ is a loss function, e.g., the zero-one loss for classification $\\ell_{01}(Y,\\hat{Y}) = 1(Y \\neq \\hat{Y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7efef8f514caf78e7bc2a60b4d5c0e7fa6d160ac"
   },
   "source": [
    "<a id=\"7\"></a> <br>\n",
    "# 2-4 Applications\n",
    "\n",
    "1. **Classifying** signal from background events; \n",
    "1. **Diagnosing** disease from symptoms;\n",
    "1. **Recognising** cats in pictures;\n",
    "1. **Identifying** body parts with Kinect cameras;\n",
    "- ...\n",
    "  ###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7cc13baab79cbc6446763e4ebe8feba2c95e74c9"
   },
   "source": [
    "<a id=\"8\"></a> <br>\n",
    "# 2-5 How to use Sklearn Data Set? \n",
    "\n",
    "- Input data = Numpy arrays or Scipy sparse matrices ;\n",
    "- Algorithms are expressed using high-level operations defined on matrices or vectors (similar to MATLAB) ;\n",
    "    - Leverage efficient low-leverage implementations ;\n",
    "    - Keep code short and readable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ea74e169f182b48bc12abc501df217e7c711157c"
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "X, y = make_blobs(n_samples=1000, centers=20, random_state=123)\n",
    "labels = [\"b\", \"r\"]\n",
    "y = np.take(labels, (y < 10))\n",
    "print(X) \n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9430d1ac40a1d7ba715347c27039b9b0859e674a"
   },
   "outputs": [],
   "source": [
    "# X is a 2 dimensional array, with 1000 rows and 2 columns\n",
    "print(X.shape)\n",
    " \n",
    "# y is a vector of 1000 elements\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0e4d94f4cde57a7f8aeaec876d0020b144fd7818"
   },
   "outputs": [],
   "source": [
    "# Rows and columns can be accessed with lists, slices or masks\n",
    "print(X[[1, 2, 3]])     # rows 1, 2 and 3\n",
    "print(X[:5])            # 5 first rows\n",
    "print(X[500:510, 0])    # values from row 500 to row 510 at column 0\n",
    "print(X[y == \"b\"][:5])  # 5 first rows for which y is \"b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "f87a77afd2c6d75c7d20390394a1f9ae569a30dc"
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"figure.max_open_warning\"] = -1\n",
    "plt.figure()\n",
    "for label in labels:\n",
    "    mask = (y == label)\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], c=label)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "71b52d10786896ab758025f670f7dc5c80db03a4"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "data = load_wine()\n",
    "data.target[[10, 80, 140]]\n",
    "list(data.target_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af3faf3d05bc406b6d882f527f5f5637c4e572a8"
   },
   "source": [
    "<a id=\"9\"></a> <br>\n",
    "# 2-6 Loading external data\n",
    "\n",
    "1. Numpy provides some [simple tools](https://docs.scipy.org/doc/numpy/reference/routines.io.html) for loading data from files (CSV, binary, etc);\n",
    "\n",
    "1. For structured data, Pandas provides more [advanced tools](http://pandas.pydata.org/pandas-docs/stable/io.html) (CSV, JSON, Excel, HDF5, SQL, etc);\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db67b8ed29fd65ec13569321eb9aed2edb1fec80"
   },
   "source": [
    "## 2-7 what is new?\n",
    "A new clustering algorithm: cluster.**OPTICS**: an algoritm related to cluster.**DBSCAN**, that has hyperparameters easier to set and that scales better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "5294a607209dc1d413e84d9e182334a41fa5ee08"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [2, 2], [2, 3],[8, 7], [8, 8], [25, 80]])\n",
    "clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "8e49459a594753031d57b263ee211615e978c04f"
   },
   "outputs": [],
   "source": [
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a409afb829be4b4c983398fdd08b816fefd50331"
   },
   "outputs": [],
   "source": [
    "clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "722d3cb2f6784dc78d5c800492bbde2594c2211f"
   },
   "source": [
    "## 2-8 Tip & Trick\n",
    "In this section we gather some useful advice and tools that may increase your quality-of-life when reviewing pull requests, running unit tests, and so forth. Some of these tricks consist of userscripts that require a browser extension such as TamperMonkey or GreaseMonkey; to set up userscripts you must have one of these extensions installed, enabled and running. We provide userscripts as GitHub gists; to install them, click on the “Raw” button on the gist page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "50c34f49740671a08c044f413ec1a258b81da727"
   },
   "source": [
    "### 2-8-1 Profiling Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ecd1c1a327756d112f562d05e894031cd498133"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.datasets import load_digits\n",
    "X = load_digits().data\n",
    "%timeit NMF(n_components=16, tol=1e-2).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "72cc7c7b60a33390a85b16bc34e3b9e424650cdd"
   },
   "source": [
    "<a id=\"10\"></a> <br>\n",
    "# 2-9 Model Deployment\n",
    "All learning algorithms in scikit-learn share a uniform and limited API consisting of complementary interfaces:\n",
    "\n",
    "1. an `estimator` interface for building and fitting models;\n",
    "1. a `predictor` interface for making predictions;\n",
    "1. a `transformer` interface for converting data.\n",
    "\n",
    "Goal: enforce a simple and consistent API to __make it trivial to swap or plug algorithms__. \n",
    "\n",
    "In this section have been applied more than **20 learning algorithms** that play an important rule in your experiences and improve your knowledge in case of using sklearn.\n",
    "\n",
    "> **<< Note 3 >>** : The results shown here may be slightly different for your analysis because, for example, the neural network algorithms use random number generators for fixing the initial value of the weights (starting points) of the neural networks, which often result in obtaining slightly different (local minima) solutions each time you run the analysis. Also note that changing the seed for the random number generator used to create the train, test, and validation samples can change your results.\n",
    "\n",
    "  ###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4b7788bbaaace438242d3b2d0d2ed489a91939ce"
   },
   "source": [
    "<a id=\"11\"></a> <br>\n",
    "## 2-10  Families of ML algorithms\n",
    "There are several categories for machine learning algorithms, below are some of these categories:\n",
    "<img src='https://i.stack.imgur.com/rLN4Z.png'>\n",
    "* Linear\n",
    "    * Linear Regression\n",
    "    * Logistic Regression\n",
    "    * Support Vector Machines\n",
    "* Tree-Based\n",
    "    * Decision Tree\n",
    "    * Random Forest\n",
    "    * GBDT\n",
    "* KNN\n",
    "* Neural Networks\n",
    "\n",
    "-----------------------------\n",
    "And if we  want to categorize ML algorithms with the type of learning, there are below type:\n",
    "* Classification\n",
    "\n",
    "    * k-Nearest \tNeighbors\n",
    "    * LinearRegression\n",
    "    * SVM\n",
    "    * DT \n",
    "    * NN\n",
    "    \n",
    "* clustering\n",
    "\n",
    "    * K-means\n",
    "    * HCA\n",
    "    * Expectation Maximization\n",
    "    \n",
    "* Visualization \tand\tdimensionality \treduction:\n",
    "\n",
    "    * Principal \tComponent \tAnalysis(PCA)\n",
    "    * Kernel PCA\n",
    "    * Locally -Linear\tEmbedding \t(LLE)\n",
    "    * t-distributed\tStochastic\tNeighbor\tEmbedding \t(t-SNE)\n",
    "    \n",
    "* Association \trule\tlearning\n",
    "\n",
    "    * Apriori\n",
    "    * Eclat\n",
    "* Semisupervised learning\n",
    "* Reinforcement Learning\n",
    "    * Q-learning\n",
    "* Batch learning & Online learning\n",
    "* Ensemble  Learning\n",
    "\n",
    "**<< Note >>**\n",
    "> Here is no method which outperforms all others for all tasks\n",
    "\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "daf9910caba26e071ff560dbdaca079ee148e140"
   },
   "source": [
    "<a id=\"12\"></a> <br>\n",
    "## 2-11 Prepare Features & Targets\n",
    "First of all seperating the data into dependent(**Feature**) and independent(**Target**) variables.\n",
    "\n",
    "**<< Note 4 >>**\n",
    "1. X==>>Feature\n",
    "1. y==>>Target\n",
    "##  Test error\n",
    "\n",
    "Issue: the training error is a __biased__ estimate of the generalization error.\n",
    "\n",
    "Solution: Divide ${\\cal L}$ into two disjoint parts called training and test sets (usually using 70% for training and 30% for test).\n",
    "1. Use the training set for fitting the model;\n",
    "1. Use the test set for evaluation only, thereby yielding an unbiased estimate.\n",
    "\n",
    "  ###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b06cb1191a0f52a904c52a918d1f999536e79bda"
   },
   "outputs": [],
   "source": [
    "\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d13f167dd92888d856c4ad2ff2895bf4855e361c"
   },
   "source": [
    "<a id=\"13\"></a> <br>\n",
    "## 2-12 Accuracy and precision\n",
    "- Recall that we want to learn an estimator $\\varphi$ minimizing the generalization error $Err(\\varphi) = \\mathbb{E}_{X,Y}\\{ \\ell(Y, \\varphi(X)) \\}$.\n",
    "\n",
    "- Problem: Since $P_{X,Y}$ is unknown, the generalization error $Err(\\varphi)$ cannot be evaluated.\n",
    "\n",
    "- Solution: Use a proxy to approximate $Err(\\varphi)$.\n",
    "* **precision** : \n",
    "\n",
    "In pattern recognition, information retrieval and binary classification, precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, \n",
    "* **recall** : \n",
    "\n",
    "recall is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. \n",
    "* **F-score** :\n",
    "\n",
    "the F1 score is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n",
    "**What is the difference between accuracy and precision?**\n",
    "\"Accuracy\" and \"precision\" are general terms throughout science. A good way to internalize the difference are the common \"bullseye diagrams\". In machine learning/statistics as a whole, accuracy vs. precision is analogous to bias vs. variance.\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc2471a2bc5d24fbee0532a71219e8b25996c20c"
   },
   "source": [
    "<a id=\"14\"></a> <br>\n",
    "## 2-13 Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ae2c9909a05b213a567338be03f0b880dcbc42fd"
   },
   "outputs": [],
   "source": [
    "class Estimator(object):\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fits estimator to data.\"\"\"\n",
    "        # set state of ``self``\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c7c25b799b47dd172f3f73e2c85d2670b0095124"
   },
   "outputs": [],
   "source": [
    "# Import the nearest neighbor class\n",
    "from sklearn.neighbors import KNeighborsClassifier  # Change this to try \n",
    "                                                    # something else\n",
    "\n",
    "# Set hyper-parameters, for controlling algorithm\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Learn a model from training data\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "f3813605cc909c6fecd52a12e2edc37ad4cc42c2"
   },
   "outputs": [],
   "source": [
    "# Estimator state is stored in instance attributes\n",
    "clf._tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0984dea25e9a6c6c7b7372057f87c4bacd230375"
   },
   "source": [
    "<a id=\"15\"></a> <br>\n",
    "## 2-14 Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9ca84979bfe46e0b6ce79ade03ac19efde72f5c3"
   },
   "outputs": [],
   "source": [
    "# Make predictions  \n",
    "print(clf.predict(X[:5])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "1b12ceaf8ca499e5e7d3486955f281f5bd72f34b"
   },
   "outputs": [],
   "source": [
    "# Compute (approximate) class probabilities\n",
    "print(clf.predict_proba(X[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b8b544762cc789bfeb8ebccd6765f77b9c7e1a0f"
   },
   "source": [
    "<a id=\"16\"></a> <br>\n",
    "## 2-15 K-Nearest Neighbours\n",
    "In **Machine Learning**, the **k-nearest neighbors algorithm** (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n",
    "\n",
    "In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "In k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.\n",
    "k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "eaa2caacfbc319932f79c75c549364089d1e649f"
   },
   "outputs": [],
   "source": [
    "# K-Nearest Neighbours\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "Model = KNeighborsClassifier(n_neighbors=8)\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e01bbec9f80532e30c6cf26d5c3fffffb5ea01d4"
   },
   "source": [
    "<a id=\"17\"></a> <br>\n",
    "##  2-16 Radius Neighbors Classifier\n",
    "Classifier implementing a **vote** among neighbors within a given **radius**\n",
    "\n",
    "In scikit-learn **RadiusNeighborsClassifier** is very similar to **KNeighborsClassifier** with the exception of two parameters. First, in RadiusNeighborsClassifier we need to specify the radius of the fixed area used to determine if an observation is a neighbor using radius. Unless there is some substantive reason for setting radius to some value, it is best to treat it like any other hyperparameter and tune it during model selection. The second useful parameter is outlier_label, which indicates what label to give an observation that has no observations within the radius - which itself can often be a useful tool for identifying outliers.\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "7728fdafa163e068668cea92cf8d79306b41d458"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import  RadiusNeighborsClassifier\n",
    "Model=RadiusNeighborsClassifier(radius=8.0)\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "#summary of the predictions made by the classifier\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "#Accouracy score\n",
    "print('accuracy is ', accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e55a785373bf654e0d4b2a78693fab1c8a625acb"
   },
   "source": [
    "<a id=\"18\"></a> <br>\n",
    "## 2-17 Logistic Regression\n",
    "Logistic regression is the appropriate regression analysis to conduct when the dependent variable is **dichotomous** (binary). Like all regression analyses, the logistic regression is a **predictive analysis**.\n",
    "\n",
    "In statistics, the logistic model (or logit model) is a widely used statistical model that, in its basic form, uses a logistic function to model a binary dependent variable; many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model; it is a form of binomial regression. Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail, win/lose, alive/dead or healthy/sick; these are represented by an indicator variable, where the two values are labeled \"0\" and \"1\"\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "55eb348cf69272192274cd0728a123796b459b55"
   },
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "Model = LogisticRegression()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c0a1c2ccaa4f6e9c5e2e42c47a295ceef7abd3b9"
   },
   "source": [
    "<a id=\"19\"></a> <br>\n",
    "##  2-18 Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "d509b2111a143660dd5cb1f02ea2779e38295b77"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "Model = PassiveAggressiveClassifier()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52938b49082dac7b35dc627828838bf12924cc7f"
   },
   "source": [
    "<a id=\"20\"></a> <br>\n",
    "## 2-19 Naive Bayes\n",
    "In machine learning, naive Bayes classifiers are a family of simple \"**probabilistic classifiers**\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "522d4a3fa874950d0850a5a9a4178ec763781ec3"
   },
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "Model = GaussianNB()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e530d18ab308e36d575806583e534cc07fe61c61"
   },
   "source": [
    "<a id=\"21\"></a> <br>\n",
    "##  2-20 BernoulliNB\n",
    "Like MultinomialNB, this classifier is suitable for **discrete data**. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "e7051b5e9aa144b74e9913cb2a6668832e7f3e02"
   },
   "outputs": [],
   "source": [
    "# BernoulliNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "Model = BernoulliNB()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "386d2d0e4fc7f5dc2b9298226d8e2ecfb7150346"
   },
   "source": [
    "<a id=\"22\"></a> <br>\n",
    "## 2-21 SVM\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "* Effective in high dimensional spaces.\n",
    "* Still effective in cases where number of dimensions is greater than the number of samples. \n",
    "* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a44a5a43945404c95863668c2ba099f6032357f8"
   },
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "Model = SVC()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b1d092cc03dcaa712f4fe4ec6867b292321377d5"
   },
   "source": [
    "<a id=\"23\"></a> <br>\n",
    "## 2-22 Nu-Support Vector Classification\n",
    "\n",
    "> Similar to SVC but uses a parameter to control the number of support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "2fa7c9a5bef780adb400bd9ad83d030f83a8d2b3"
   },
   "outputs": [],
   "source": [
    "# Support Vector Machine's \n",
    "from sklearn.svm import NuSVC\n",
    "\n",
    "Model = NuSVC()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5d07a75e83251ddbf8cfdfd11c9faa2671ad87ff"
   },
   "source": [
    "<a id=\"24\"></a> <br>\n",
    "## 2-23 Linear Support Vector Classification\n",
    "\n",
    "Similar to **SVC** with parameter kernel=’linear’, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "7e7275f160f2e4e270200eaa01c13be5cb465142"
   },
   "outputs": [],
   "source": [
    "# Linear Support Vector Classification\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "Model = LinearSVC()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cec81c9e0c3bc6afba07811a321b5383a0f823f3"
   },
   "source": [
    "<a id=\"25\"></a> <br>\n",
    "## 2-24 Decision Tree\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for **classification** and **regression**. The goal is to create a model that predicts the value of a target variable by learning simple **decision rules** inferred from the data features.\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "10e25ad67f7c25a8654637d4ba496b64121d67d0"
   },
   "outputs": [],
   "source": [
    "# Decision Tree's\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "Model = DecisionTreeClassifier()\n",
    "\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a7d897130fd705943764e924bbe468c99b7c036a"
   },
   "source": [
    "<a id=\"26\"></a> <br>\n",
    "## 2-25 ExtraTreeClassifier\n",
    "An extremely randomized tree classifier.\n",
    "\n",
    "Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the **max_features** randomly selected features and the best split among those is chosen. When max_features is set 1, this amounts to building a totally random decision tree.\n",
    "\n",
    "**Warning**: Extra-trees should only be used within ensemble methods.\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "5a775006a814b6aacdcc07dc46995eb291b873f1"
   },
   "outputs": [],
   "source": [
    "# ExtraTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "Model = ExtraTreeClassifier()\n",
    "\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "48f940f73580a2997d75f22eba09d938c86a1a97"
   },
   "source": [
    "<a id=\"27\"></a> <br>\n",
    "## 2-26 Neural network\n",
    "\n",
    "I have used multi-layer Perceptron classifier.\n",
    "This model optimizes the log-loss function using **LBFGS** or **stochastic gradient descent**.\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1c10482510f654878f93b573dc1abe4112b861eb"
   },
   "source": [
    "<a id=\"28\"></a> <br>\n",
    "## 2-26-1 What is a Perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a94d82b497cbe543da0a637ecfed6e9e8b7569e7"
   },
   "source": [
    "There are many online examples and tutorials on perceptrons and learning. Here is a list of some articles:\n",
    "- [Wikipedia on Perceptrons](https://en.wikipedia.org/wiki/Perceptron)\n",
    "- Jurafsky and Martin (ed. 3), Chapter 8\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8e4da1a0b3d51a5fff38750fb4631ac3aa7eebb"
   },
   "source": [
    "This is an example that I have taken from a draft of the 3rd edition of Jurafsky and Martin, with slight modifications:\n",
    "We import *numpy* and use its *exp* function. We could use the same function from the *math* module, or some other module like *scipy*. The *sigmoid* function is defined as in the textbook:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "196e5a54ed0de712e2254e77439051267cad4b3d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9aa207d0bb6a7342932406d8fa68cbd49be866b5"
   },
   "source": [
    "Our example data, **weights** $w$, **bias** $b$, and **input** $x$ are defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "69cae82df8a906ad43594464c6497e05e282dcd1"
   },
   "outputs": [],
   "source": [
    "w = np.array([0.2, 0.3, 0.8])\n",
    "b = 0.5\n",
    "x = np.array([0.5, 0.6, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8cd5da02c0fc0beedd9a3417e399982293d28fd2"
   },
   "source": [
    "Our neural unit would compute $z$ as the **dot-product** $w \\cdot x$ and add the **bias** $b$ to it. The sigmoid function defined above will convert this $z$ value to the **activation value** $a$ of the unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "65d083572bf2cc897d816765db05758b107741ff"
   },
   "outputs": [],
   "source": [
    "z = w.dot(x) + b\n",
    "print(\"z:\", z)\n",
    "print(\"a:\", sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9e9101a20c9a167a1d925f3b64aafb94317155e2"
   },
   "source": [
    "<a id=\"29\"></a> <br>\n",
    "### 1-26-2 The XOR Problem\n",
    "The power of neural units comes from combining them into larger networks. Minsky and Papert (1969): A single neural unit cannot compute the simple logical function XOR.\n",
    "\n",
    "The task is to implement a simple **perceptron** to compute logical operations like AND, OR, and XOR.\n",
    "\n",
    "- Input: $x_1$ and $x_2$\n",
    "- Bias: $b = -1$ for AND; $b = 0$ for OR\n",
    "- Weights: $w = [1, 1]$\n",
    "\n",
    "with the following activation function:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "    \\ 0 & \\quad \\text{if } w \\cdot x + b \\leq 0\\\\\n",
    "    \\ 1 & \\quad \\text{if } w \\cdot x + b > 0\n",
    "  \\end{cases}\n",
    "$$\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c03b3a6a5307cc53e637afd6da5c307c985be7ac"
   },
   "source": [
    "We can define this activation function in Python as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b0832df65fc1aa694a1cf67b8713c894b1ed2a2"
   },
   "outputs": [],
   "source": [
    "def activation(z):\n",
    "    if z > 0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "27ae56a47bd180ebbfd82b3f0242c8078db6ac97"
   },
   "source": [
    "For AND we could implement a perceptron as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "3267d3f1a2864f29ff2382a2057b0b88fa74b649"
   },
   "outputs": [],
   "source": [
    "w = np.array([1, 1])\n",
    "b = -1\n",
    "x = np.array([0, 0])\n",
    "print(\"0 AND 0:\", activation(w.dot(x) + b))\n",
    "x = np.array([1, 0])\n",
    "print(\"1 AND 0:\", activation(w.dot(x) + b))\n",
    "x = np.array([0, 1])\n",
    "print(\"0 AND 1:\", activation(w.dot(x) + b))\n",
    "x = np.array([1, 1])\n",
    "print(\"1 AND 1:\", activation(w.dot(x) + b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dbee1f41f0bd66613f5b49e0f65be4bfd9f91283"
   },
   "source": [
    "For OR we could implement a perceptron as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "bf47440fb2f5a0fb016286f0a3a41a05a8416495"
   },
   "outputs": [],
   "source": [
    "w = np.array([1, 1])\n",
    "b = 0\n",
    "x = np.array([0, 0])\n",
    "print(\"0 OR 0:\", activation(w.dot(x) + b))\n",
    "x = np.array([1, 0])\n",
    "print(\"1 OR 0:\", activation(w.dot(x) + b))\n",
    "x = np.array([0, 1])\n",
    "print(\"0 OR 1:\", activation(w.dot(x) + b))\n",
    "x = np.array([1, 1])\n",
    "print(\"1 OR 1:\", activation(w.dot(x) + b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "181068ef1b8e1ba568093184c41a118b4c0bfe7f"
   },
   "source": [
    "There is no way to implement a perceptron for XOR this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d12494da861ea094378c7cf6a3409803fb5585ac"
   },
   "source": [
    "no see our prediction for iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "5f040cfaeb71f8caa94e4d7f18cccde8d2a0b8a7"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "Model=MLPClassifier()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "# Summary of the predictions\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ffc339dbf9c8da74194b994930694bd97bb2afbb"
   },
   "source": [
    "<a id=\"30\"></a> <br>\n",
    "## 2-27 RandomForest\n",
    "A random forest is a meta estimator that **fits a number of decision tree classifiers** on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. \n",
    "\n",
    "The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "8ed2305b51c2248a8aa62cf4452632f448e83771"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "Model=RandomForestClassifier(max_depth=2)\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1311eb15f2afceed2219faeb859d0d07b7072176"
   },
   "source": [
    "<a id=\"31\"></a> <br>\n",
    "## 2-28 Bagging classifier \n",
    "A Bagging classifier is an ensemble **meta-estimator** that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n",
    "\n",
    "This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting . If samples are drawn with replacement, then the method is known as Bagging . When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces . Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches .[http://scikit-learn.org]\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c11c731d3db6c1c81301da85dc158cb7d324c4cb"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "Model=BaggingClassifier()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c0944bd32424f38906148d96f4b1e6fccfbf97a6"
   },
   "source": [
    "<a id=\"32\"></a> <br>\n",
    "##  2-29 AdaBoost classifier\n",
    "\n",
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n",
    "This class implements the algorithm known as **AdaBoost-SAMME** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "938946ee8e017b982c4c06e193d4d13cb7d3fb5f"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "Model=AdaBoostClassifier()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d62842d12731d3eb1d6577c5b35c12c4886c708"
   },
   "source": [
    "<a id=\"33\"></a> <br>\n",
    "## 2-30 Gradient Boosting Classifier\n",
    "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "863124561c0d1b5995d0b8d3702daa7bc364d6b0"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "Model=GradientBoostingClassifier()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e89b4494bd78c2d66beeba34a4e320fd8c9dae0c"
   },
   "source": [
    "<a id=\"34\"></a> <br>\n",
    "## 2-31 Linear Discriminant Analysis\n",
    "Linear Discriminant Analysis (discriminant_analysis.LinearDiscriminantAnalysis) and Quadratic Discriminant Analysis (discriminant_analysis.QuadraticDiscriminantAnalysis) are two classic classifiers, with, as their names suggest, a **linear and a quadratic decision surface**, respectively.\n",
    "\n",
    "These classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no **hyperparameters** to tune.\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0796cd9f1c902345df605b7557a9c3ff686e35a9"
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "Model=LinearDiscriminantAnalysis()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "296137970fc94fa4a4eb4185cb5fa952b1985c57"
   },
   "source": [
    "<a id=\"35\"></a> <br>\n",
    "## 2-32 Quadratic Discriminant Analysis\n",
    "A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.\n",
    "\n",
    "The model fits a **Gaussian** density to each class.\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "5f521d19f295b8e8f24f5715e93b1c45e9a6bce3"
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "Model=QuadraticDiscriminantAnalysis()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0518634bf8850ac1bfcfc301e93a8740e1995c3a"
   },
   "source": [
    "<a id=\"36\"></a> <br>\n",
    "## 2-33 Kmeans \n",
    "K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). \n",
    "\n",
    "The goal of this algorithm is **to find groups in the data**, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided.\n",
    "\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "427f08af02fc7288a5e35de5ff4b6c33b8fce491"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "iris_SP = dataset[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n",
    "# k-means cluster analysis for 1-15 clusters                                              \n",
    "from scipy.spatial.distance import cdist\n",
    "clusters=range(1,15)\n",
    "meandist=[]\n",
    "\n",
    "# loop through each cluster and fit the model to the train set\n",
    "# generate the predicted cluster assingment and append the mean \n",
    "# distance my taking the sum divided by the shape\n",
    "for k in clusters:\n",
    "    model=KMeans(n_clusters=k)\n",
    "    model.fit(iris_SP)\n",
    "    clusassign=model.predict(iris_SP)\n",
    "    meandist.append(sum(np.min(cdist(iris_SP, model.cluster_centers_, 'euclidean'), axis=1))\n",
    "    / iris_SP.shape[0])\n",
    "\n",
    "\"\"\"\n",
    "Plot average distance from observations from the cluster centroid\n",
    "to use the Elbow Method to identify number of clusters to choose\n",
    "\"\"\"\n",
    "plt.plot(clusters, meandist)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average distance')\n",
    "plt.title('Selecting k with the Elbow Method') \n",
    "# pick the fewest number of clusters that reduces the average distance\n",
    "# If you observe after 3 we can see graph is almost linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7dfdff6d8a54c846d12a3c234e7765bb7f9d06f3"
   },
   "source": [
    "<a id=\"37\"></a> <br>\n",
    "## 2-34 Plot classification probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "f8e04572a7c768f5928229a8896dc65008571cbc"
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, 0:2]  # we only take the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "n_features = X.shape[1]\n",
    "\n",
    "C = 10\n",
    "kernel = 1.0 * RBF([1.0, 1.0])  # for GPC\n",
    "\n",
    "# Create different classifiers.\n",
    "classifiers = {\n",
    "    'L1 logistic': LogisticRegression(C=C, penalty='l1',\n",
    "                                      solver='saga',\n",
    "                                      multi_class='multinomial',\n",
    "                                      max_iter=10000),\n",
    "    'L2 logistic (Multinomial)': LogisticRegression(C=C, penalty='l2',\n",
    "                                                    solver='saga',\n",
    "                                                    multi_class='multinomial',\n",
    "                                                    max_iter=10000),\n",
    "    'L2 logistic (OvR)': LogisticRegression(C=C, penalty='l2',\n",
    "                                            solver='saga',\n",
    "                                            multi_class='ovr',\n",
    "                                            max_iter=10000),\n",
    "    'Linear SVC': SVC(kernel='linear', C=C, probability=True,\n",
    "                      random_state=0),\n",
    "    'GPC': GaussianProcessClassifier(kernel)\n",
    "}\n",
    "\n",
    "n_classifiers = len(classifiers)\n",
    "\n",
    "plt.figure(figsize=(3 * 2, n_classifiers * 2))\n",
    "plt.subplots_adjust(bottom=.2, top=.95)\n",
    "\n",
    "xx = np.linspace(3, 9, 100)\n",
    "yy = np.linspace(1, 5, 100).T\n",
    "xx, yy = np.meshgrid(xx, yy)\n",
    "Xfull = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "for index, (name, classifier) in enumerate(classifiers.items()):\n",
    "    classifier.fit(X, y)\n",
    "\n",
    "    y_pred = classifier.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    print(\"Accuracy (train) for %s: %0.1f%% \" % (name, accuracy * 100))\n",
    "\n",
    "    # View probabilities:\n",
    "    probas = classifier.predict_proba(Xfull)\n",
    "    n_classes = np.unique(y_pred).size\n",
    "    for k in range(n_classes):\n",
    "        plt.subplot(n_classifiers, n_classes, index * n_classes + k + 1)\n",
    "        plt.title(\"Class %d\" % k)\n",
    "        if k == 0:\n",
    "            plt.ylabel(name)\n",
    "        imshow_handle = plt.imshow(probas[:, k].reshape((100, 100)),\n",
    "                                   extent=(3, 9, 1, 5), origin='lower')\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        idx = (y_pred == k)\n",
    "        if idx.any():\n",
    "            plt.scatter(X[idx, 0], X[idx, 1], marker='o', c='w', edgecolor='k')\n",
    "\n",
    "ax = plt.axes([0.15, 0.04, 0.7, 0.05])\n",
    "plt.title(\"Probability\")\n",
    "plt.colorbar(imshow_handle, cax=ax, orientation='horizontal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c5048b61a4837c8826551c8871609973ebbe3847"
   },
   "source": [
    "<a id=\"38\"></a> <br>\n",
    "# 3- NLTK\n",
    "The Natural Language Toolkit (NLTK) is one of the leading platforms for working with human language data and Python, the module NLTK is used for natural language processing. NLTK is literally an acronym for Natural Language Toolkit. with it you can tokenizing words and sentences.\n",
    "NLTK is a library of Python that can mine (scrap and upload data) and analyse very large amounts of textual data using computational methods.\n",
    "<img src='https://arts.unimelb.edu.au/__data/assets/image/0005/2735348/nltk.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "adadeb7a83d0bc711a779948197c40841b10f1ca"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    " \n",
    "data = \"All work and no play makes jack a dull boy, all work and no play\"\n",
    "print(word_tokenize(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e49407d9fe96b86c9851fbd7065ebfb218281687"
   },
   "source": [
    "<a id=\"39\"></a> <br>\n",
    "All of them are words except the comma. Special characters are treated as separate tokens.\n",
    "\n",
    "## 3-1 Tokenizing sentences\n",
    "The same principle can be applied to sentences. Simply change the to sent_tokenize()\n",
    "We have added two sentences to the variable data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ec9e6c715a1d49b2813c934fb4405ccec77884a1"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    " \n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "print(sent_tokenize(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "75bb691fdb4982097ee8eb59ae930c1d81074afa"
   },
   "source": [
    "<a id=\"40\"></a> <br>\n",
    "## 3-2 NLTK and arrays\n",
    "If you wish to you can store the words and sentences in arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "fde02d4189b0a52b7f919ac0fa0643d84ebacaf7"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    " \n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    " \n",
    "phrases = sent_tokenize(data)\n",
    "words = word_tokenize(data)\n",
    " \n",
    "print(phrases)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7fad127cb8ed99cc063b98b3391645263737958"
   },
   "source": [
    "<a id=\"41\"></a> <br>\n",
    "## 3-3 NLTK stop words\n",
    "Natural language processing (nlp) is a research field that presents many challenges such as natural language understanding.\n",
    "Text may contain stop words like ‘the’, ‘is’, ‘are’. Stop words can be filtered from the text to be processed. There is no universal list of stop words in nlp research, however the nltk module contains a list of stop words.\n",
    "\n",
    "In this article you will learn how to remove stop words with the nltk module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "3357ec158943478d584c392bb7702fe7e6d4b355"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "stopWords = set(stopwords.words('english'))\n",
    "words = word_tokenize(data)\n",
    "wordsFiltered = []\n",
    " \n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    " \n",
    "print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "581a7ba2ce1ae5dae6c36d54f8999af838c7b80c"
   },
   "source": [
    "A module has been imported:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2cb63648a5f138fe779744f0c52d570f30f84b13"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e7528080723ea540729e78b6e135475a870a5618"
   },
   "source": [
    "We get a set of English stop words using the line:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6fbe468728072fb1883e064d0c1e892259fb1c0c"
   },
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "43843cdaccbe961422631c13d982e13bf25607c6"
   },
   "source": [
    "The returned list stopWords contains 153 stop words on my computer.\n",
    "You can view the length or contents of this array with the lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53582f8f5ae2871e2cbba4542fc38965b61a5012"
   },
   "outputs": [],
   "source": [
    "print(len(stopWords))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d25fed9ed1fd0016cea56de8e71b010a0d3176c3"
   },
   "source": [
    "We create a new list called wordsFiltered which contains all words which are not stop words.\n",
    "To create it we iterate over the list of words and only add it if its not in the stopWords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b28823a9862bb263183c6d62a4e91286bfe8d30"
   },
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8546a903a5b3916bca4feb96116dd00db1fc51c0"
   },
   "source": [
    "<a id=\"42\"></a> <br>\n",
    "## 3-4 NLTK – stemming\n",
    "Start by defining some words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f3a8e1427f235183fc5b9656a15a1cdb9befb55b"
   },
   "outputs": [],
   "source": [
    "words = [\"game\",\"gaming\",\"gamed\",\"games\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a6ea3737dbb032d08c896f24cc6555b7b516274"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4bd4f0bc00227a77cfe734575ebfa7d124d333d4"
   },
   "source": [
    "And stem the words in the list using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "12b5f99ca390ed21e68862e5eb5968d31e3858ef"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "words = [\"game\",\"gaming\",\"gamed\",\"games\"]\n",
    "ps = PorterStemmer()\n",
    " \n",
    "for word in words:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b0af11da2319b2be2643ad0003ebbc207067dc34"
   },
   "source": [
    "<a id=\"43\"></a> <br>\n",
    "## 3-5 NLTK speech tagging\n",
    "The module NLTK can automatically tag speech.\n",
    "Given a sentence or paragraph, it can label words such as verbs, nouns and so on.\n",
    "\n",
    "NLTK – speech tagging example\n",
    "The example below automatically tags words with a corresponding class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe586131bec724c1901a441b56753c1d47562483"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    " \n",
    "document = 'Whether you\\'re new to programming or an experienced developer, it\\'s easy to learn and use Python.'\n",
    "sentences = nltk.sent_tokenize(document)   \n",
    "for sent in sentences:\n",
    "    print(nltk.pos_tag(nltk.word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22e469c3b880de5c4f00a609192e74d97d22436a"
   },
   "source": [
    "We can filter this data based on the type of word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8edca0d46e25f4d8dba85c454bb70299b7c1e112"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    " \n",
    "document = 'Today the Netherlands celebrates King\\'s Day. To honor this tradition, the Dutch embassy in San Francisco invited me to'\n",
    "sentences = nltk.sent_tokenize(document)   \n",
    " \n",
    "data = []\n",
    "for sent in sentences:\n",
    "    data = data + nltk.pos_tag(nltk.word_tokenize(sent))\n",
    " \n",
    "for word in data: \n",
    "    if 'NNP' in word[1]: \n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "488b9eadaebb4fb6ae4585e991b9d1d6af176490"
   },
   "source": [
    "<a id=\"44\"></a> <br>\n",
    "## 3-6 Natural Language Processing – prediction\n",
    "We can use natural language processing to make predictions. Example: Given a product review, a computer can predict if its positive or negative based on the text. In this article you will learn how to make a prediction program based on natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6dd913ea82ac7dcc478ab861e92e483a57beca0"
   },
   "source": [
    "<a id=\"45\"></a> <br>\n",
    "### 3-6-1 nlp prediction example\n",
    "Given a name, the classifier will predict if it’s a male or female.\n",
    "\n",
    "To create our analysis program, we have several steps:\n",
    "\n",
    "1. Data preparation\n",
    "1. Feature extraction\n",
    "1. Training\n",
    "1. Prediction\n",
    "1. Data preparation\n",
    "The first step is to prepare data. We use the names set included with nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73da028c3418b256653ec37d516b667af5541225"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    " \n",
    "# Load data and training \n",
    "names = ([(name, 'male') for name in names.words('male.txt')] + \n",
    "\t [(name, 'female') for name in names.words('female.txt')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3421ce619481bc3f4b8817fd14c3c33374975c9b"
   },
   "source": [
    "This dataset is simply a collection of tuples. To give you an idea of what the dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd30af0cc73300d0d83c7d525af3281bfc718e54"
   },
   "outputs": [],
   "source": [
    "[(u'Aaron', 'male'), (u'Abbey', 'male'), (u'Abbie', 'male')]\n",
    "[(u'Zorana', 'female'), (u'Zorina', 'female'), (u'Zorine', 'female')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "022d450b9d514737392698136ee046fbbd73597c"
   },
   "source": [
    "You can define your own set of tuples if you wish, its simply a list containing many tuples.\n",
    "\n",
    "Feature extraction\n",
    "Based on the dataset, we prepare our feature. The feature we will use is the last letter of a name:\n",
    "We define a featureset using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3745234017dc235c7e903943d2d67930402eac1a"
   },
   "source": [
    "featuresets = [(gender_features(n), g) for (n,g) in names]\n",
    "and the features (last letters) are extracted using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f3baaa6a29bbf22a153094df0bbf38c8a0ff0430"
   },
   "outputs": [],
   "source": [
    "def gender_features(word): \n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9ef5cb06006e20577878cb93d2c8c488fe341c70"
   },
   "source": [
    "Training and prediction\n",
    "We train and predict using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dba616c9612c65a875319837061d8c3861e9e6dd"
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import names\n",
    " \n",
    "def gender_features(word): \n",
    "    return {'last_letter': word[-1]} \n",
    " \n",
    "# Load data and training \n",
    "names = ([(name, 'male') for name in names.words('male.txt')] + \n",
    "\t [(name, 'female') for name in names.words('female.txt')])\n",
    " \n",
    "featuresets = [(gender_features(n), g) for (n,g) in names] \n",
    "train_set = featuresets\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) \n",
    " \n",
    "# Predict\n",
    "print(classifier.classify(gender_features('Frank')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d2d2be49b9597ad4fee4f0bc48dfd1df7934a447"
   },
   "source": [
    "If you want to give the name during runtime, change the last line to:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a55ec4316a7dd340d09b4c93f579357b2022db5f"
   },
   "outputs": [],
   "source": [
    "# Predict, you can change name\n",
    "name = 'Sarah'\n",
    "print(classifier.classify(gender_features(name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f7e32d6aeeed974e9c9d23189241aa17ca69de3"
   },
   "source": [
    "<a id=\"45\"></a> <br>\n",
    "## 3-7 Python Sentiment Analysis\n",
    "In Natural Language Processing there is a concept known as Sentiment Analysis.\n",
    "\n",
    "Given a movie review or a tweet, it can be automatically classified in categories.\n",
    "These categories can be user defined (positive, negative) or whichever classes you want.\n",
    "Classification is done using several steps: training and prediction.\n",
    "\n",
    "The training phase needs to have training data, this is example data in which we define examples. The classifier will use the training data to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "180a352f2870cf979345790226b4d4bfe813c886"
   },
   "source": [
    "We start by defining 3 classes: positive, negative and neutral.\n",
    "Each of these is defined by a vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "199b8ebaa48d358cf2a2520d4ee1b8edcf22f81c"
   },
   "outputs": [],
   "source": [
    "positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\n",
    "negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\n",
    "neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f50e153d90eb5519f6c76e487ac7c0212c9dd0ee"
   },
   "source": [
    "Every word is converted into a feature using a simplified bag of words model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5bba07df26463ff90b6d71a551f8aae0960de7e0"
   },
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "neutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e02426d80adacc804fc5497c9e3d7607e8cc030"
   },
   "source": [
    "Our training set is then the sum of these three feature sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c9f2545fa20caca3034dbb62521e064ff4d322b6"
   },
   "outputs": [],
   "source": [
    "train_set = negative_features + positive_features + neutral_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf69fcf2e05e3a55c05c1f62adaa74c8cebfcf45"
   },
   "source": [
    "We train the classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0577b4e7dcb13b48a0918c7b632716482e9c042e"
   },
   "source": [
    "classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "97abd270ce5492d1d178858fc1b7f2ef3eb71c69"
   },
   "source": [
    "This example classifies sentences according to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6214a67beb47748105aa7f0c51201925f39e9e7b"
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import names\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\n",
    "negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\n",
    "neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]\n",
    " \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "neutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]\n",
    " \n",
    "train_set = negative_features + positive_features + neutral_features\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set) \n",
    " \n",
    "# Predict\n",
    "neg = 0\n",
    "pos = 0\n",
    "sentence = \"Awesome movie, I liked it\"\n",
    "sentence = sentence.lower()\n",
    "words = sentence.split(' ')\n",
    "for word in words:\n",
    "    classResult = classifier.classify( word_feats(word))\n",
    "    if classResult == 'neg':\n",
    "        neg = neg + 1\n",
    "    if classResult == 'pos':\n",
    "        pos = pos + 1\n",
    " \n",
    "print('Positive: ' + str(float(pos)/len(words)))\n",
    "print('Negative: ' + str(float(neg)/len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9b5bef86a8db19453817ecce661079eb4cad4fa8"
   },
   "source": [
    " <a id=\"46\"></a> <br>\n",
    " # 4- Read more\n",
    " \n",
    " you can start to learn and review your knowledge about ML with a perfect dataset and try to learn and memorize the workflow for your journey in Data science world with read more sources, here I want to give some courses, e-books and cheatsheet:\n",
    " ## 4-1 Courses\n",
    " \n",
    "There are a lot of online courses that can help you develop your knowledge, here I have just  listed some of them:\n",
    "\n",
    "1. [Machine Learning Certification by Stanford University (Coursera)](https://www.coursera.org/learn/machine-learning/)\n",
    "\n",
    "2. [Machine Learning A-Z™: Hands-On Python & R In Data Science (Udemy)](https://www.udemy.com/machinelearning/)\n",
    "\n",
    "3. [Deep Learning Certification by Andrew Ng from deeplearning.ai (Coursera)](https://www.coursera.org/specializations/deep-learning)\n",
    "\n",
    "4. [Python for Data Science and Machine Learning Bootcamp (Udemy)](Python for Data Science and Machine Learning Bootcamp (Udemy))\n",
    "\n",
    "5. [Mathematics for Machine Learning by Imperial College London](https://www.coursera.org/specializations/mathematics-machine-learning)\n",
    "\n",
    "6. [Deep Learning A-Z™: Hands-On Artificial Neural Networks](https://www.udemy.com/deeplearning/)\n",
    "\n",
    "7. [Complete Guide to TensorFlow for Deep Learning Tutorial with Python](https://www.udemy.com/complete-guide-to-tensorflow-for-deep-learning-with-python/)\n",
    "\n",
    "8. [Data Science and Machine Learning Tutorial with Python – Hands On](https://www.udemy.com/data-science-and-machine-learning-with-python-hands-on/)\n",
    "\n",
    "9. [Machine Learning Certification by University of Washington](https://www.coursera.org/specializations/machine-learning)\n",
    "\n",
    "10. [Data Science and Machine Learning Bootcamp with R](https://www.udemy.com/data-science-and-machine-learning-bootcamp-with-r/)\n",
    "11. [Creative Applications of Deep Learning with TensorFlow](https://www.class-central.com/course/kadenze-creative-applications-of-deep-learning-with-tensorflow-6679)\n",
    "12. [Neural Networks for Machine Learning](https://www.class-central.com/mooc/398/coursera-neural-networks-for-machine-learning)\n",
    "13. [Practical Deep Learning For Coders, Part 1](https://www.class-central.com/mooc/7887/practical-deep-learning-for-coders-part-1)\n",
    "14. [Machine Learning](https://www.cs.ox.ac.uk/teaching/courses/2014-2015/ml/index.html)\n",
    "\n",
    "## 4-2 Ebooks\n",
    "\n",
    "So you love reading , here is **10 free machine learning books**\n",
    "1. [Probability and Statistics for Programmers](http://www.greenteapress.com/thinkstats/)\n",
    "2. [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/091117.pdf)\n",
    "2. [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "2. [Understanding Machine Learning](http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html)\n",
    "2. [A Programmer’s Guide to Data Mining](http://guidetodatamining.com/)\n",
    "2. [Mining of Massive Datasets](http://infolab.stanford.edu/~ullman/mmds/book.pdf)\n",
    "2. [A Brief Introduction to Neural Networks](http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-2col-dkrieselcom.pdf)\n",
    "2. [Deep Learning](http://www.deeplearningbook.org/)\n",
    "2. [Natural Language Processing with Python](https://www.researchgate.net/publication/220691633_Natural_Language_Processing_with_Python)\n",
    "2. [Machine Learning Yearning](http://www.mlyearning.org/)\n",
    "\n",
    "## 4-3 Cheat Sheets\n",
    "\n",
    "Data Science is an ever-growing field, there are numerous tools & techniques to remember. It is not possible for anyone to remember all the functions, operations and formulas of each concept. That’s why we have cheat sheets. But there are a plethora of cheat sheets available out there, choosing the right cheat sheet is a tough task. So, I decided to write this article.\n",
    "\n",
    "Here I have selected the cheat sheets on the following criteria: comprehensiveness, clarity, and content [26]:\n",
    "1. [Quick Guide to learn Python for Data Science ](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/Data-Science-in-Python.pdf)\n",
    "1. [Python for Data Science Cheat sheet ](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/beginners_python_cheat_sheet.pdf)\n",
    "1. [Python For Data Science Cheat Sheet NumPy](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/Numpy_Python_Cheat_Sheet.pdf)\n",
    "1. [Exploratory Data Analysis in Python]()\n",
    "1. [Data Exploration using Pandas in Python](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/Data-Exploration-in-Python.pdf)\n",
    "1. [Data Visualisation in Python](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/data-visualisation-infographics1.jpg)\n",
    "1. [Python For Data Science Cheat Sheet Bokeh](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/Python_Bokeh_Cheat_Sheet.pdf)\n",
    "1. [Cheat Sheet: Scikit Learn ](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/Scikit-Learn-Infographic.pdf)\n",
    "1. [MLalgorithms CheatSheet](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/MLalgorithms-.pdf)\n",
    "1. [Probability Basics  Cheat Sheet ](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/probability_cheatsheet.pdf)\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe3d19ff691fcc7c7edf8d2cb1224e3bdeee396e"
   },
   "source": [
    "<a id=\"47\"></a> <br>\n",
    "# 5- conclusion\n",
    "After the first version of this kernel, in the second edition, we introduced Sklearn, NLTK libraries. in addition,  we examined each one in detail. this kernel it is not completed yet! Following up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a8424e6f84874112757040d36b93542a2e5ba8cb"
   },
   "source": [
    ">###### you may  be interested have a look at it: [**10-steps-to-become-a-data-scientist**](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------\n",
    "you can Fork and Run this kernel on Github:\n",
    "> ###### [ GitHub](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    " **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated**\n",
    " \n",
    " -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1923ba01df86012077df2a2750b92ebb2adb8236"
   },
   "source": [
    "<a id=\"48\"></a> <br>\n",
    "# 6- References\n",
    "1. [Coursera](https://www.coursera.org/specializations/data-science-python)\n",
    "1. [GitHub](https://github.com/mjbahmani)\n",
    "1. [Sklearn](https://scikit-learn.org)\n",
    "1. [NLTK](https://pythonspot.com/category/nltk/)\n",
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f0644ae4e74da4a20cba4e9094ed2458be44361"
   },
   "source": [
    "#### This Kernel is not completed and will be updated soon!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
